{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fa76812-fd6f-4853-99d8-9b982328bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as mt\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Data Preprocessing - Standardization, Encoding, Imputation\n",
    "from sklearn.preprocessing import StandardScaler # Standardization\n",
    "from sklearn.preprocessing import Normalizer # Normalization\n",
    "from sklearn.preprocessing import OneHotEncoder # One-hot Encoding\n",
    "from sklearn.preprocessing import OrdinalEncoder # Ordinal Encoding\n",
    "from category_encoders import MEstimateEncoder # Target Encoding\n",
    "from sklearn.preprocessing import PolynomialFeatures # Create Polynomial Features\n",
    "from sklearn.impute import SimpleImputer # Imputation\n",
    "\n",
    "# Exploratory Data Analysis - Feature Engineering\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modeling - ML Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Modeling - Algorithms\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "#from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# ML - Evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# ML - Tuning\n",
    "import optuna\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Settings\n",
    "# Settings for Seaborn\n",
    "sns.set_theme(context='notebook', style='ticks', palette=\"bwr_r\", font_scale=0.7, rc={\"figure.dpi\":240, 'savefig.dpi':240})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14eb99b6-cdfd-4b02-bbcf-8635cd06f728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/run-DataSink0-1-part-r-00000\n",
      "./data/reviews.csv\n",
      "./data/.DS_Store\n",
      "./data/listings.csv\n",
      "./data/calendar.csv\n",
      "./data/listings.json\n",
      "./data/202107/calendar.csv.gz\n",
      "./data/202107/reviews.csv\n",
      "./data/202107/.DS_Store\n",
      "./data/202107/neighbourhoods.geojson\n",
      "./data/202107/neighbourhoods.csv\n",
      "./data/202107/listings.csv\n",
      "./data/202107/reviews_summary.csv\n",
      "./data/202107/listings_summary.csv\n",
      "./data/202107/.ipynb_checkpoints/listings-checkpoint.csv\n",
      "./data/.ipynb_checkpoints/listings-checkpoint.json\n",
      "./data/.ipynb_checkpoints/run-DataSink0-1-part-r-00000-checkpoint\n",
      "./data/.ipynb_checkpoints/listings-checkpoint.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "kaggle_project = 'settle-airbnb'\n",
    "# Import dataset from local directory './data' or from Kaggle\n",
    "data_dir = ('./data' if os.path.exists('data') else f'/kaggle/input/{kaggle_project}')\n",
    "\n",
    "# print all files in data_dir\n",
    "for dirname, _, filenames in os.walk(data_dir):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Import three datasets\n",
    "review = pd.read_csv(f'{data_dir}/reviews.csv')\n",
    "listings = pd.read_csv(f'{data_dir}/listings.csv')\n",
    "calendar = pd.read_csv(f'{data_dir}/calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2e3ef9-5464-4656-aff7-154bca3ddf5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test\\netlLogger.debug(\\'This is a debug message\\')\\netlLogger.warning(\"This is a warning message\")\\netlLogger.error(\"This is an error message\")\\n!cat logs/etl.log\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import logging.config\n",
    "\n",
    "logging.config.fileConfig(\n",
    "    fname='config/etlLogger.conf', # read etlLogger.conf\n",
    "    disable_existing_loggers=False # don't disable the existing_loggers (other loggers)\n",
    ")\n",
    "\n",
    "etlLogger = logging.getLogger('etlLogger')\n",
    "\"\"\"test\n",
    "etlLogger.debug('This is a debug message')\n",
    "etlLogger.warning(\"This is a warning message\")\n",
    "etlLogger.error(\"This is an error message\")\n",
    "!cat logs/etl.log\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa8098b-04d0-48ae-aa02-738e88e243fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ba5982-2622-43dc-a07a-430ee4799c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-40e0e86f7f94>:48: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  amenities_enc[amenity] = listings.amenities.str.contains(amenity)\n"
     ]
    }
   ],
   "source": [
    "# Parse listings\n",
    "\n",
    "##\n",
    "#revise\n",
    "###\n",
    "\n",
    "def parse_listings():\n",
    "    \n",
    "    #\n",
    "    # Data type transformation\n",
    "    #\n",
    "    # Convert dollar columns from object to float\n",
    "    # Convert '$' and ',' to ''\n",
    "    dollar_cols = ['security_deposit', 'price', 'weekly_price', 'monthly_price', 'extra_people']\n",
    "    for dollar_col in dollar_cols:\n",
    "        listings[dollar_col] = listings[dollar_col].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "    #\n",
    "    # Encoding\n",
    "    #\n",
    "    # Replace amenities from {}\" to ''\n",
    "    listings.amenities.replace('[{}\"]', '', regex=True, inplace=True)\n",
    "    # Split amenities with ,\n",
    "    amenities = listings.amenities.str.split(',', expand=True)\n",
    "\n",
    "    # For each col, extract the unique amenities\n",
    "    amenities_uniques = []\n",
    "    for col in amenities.columns:\n",
    "        amenities_uniques += list(amenities[col].unique())\n",
    "\n",
    "    # Remove the duplicate values\n",
    "    amenities_uniques = set(amenities_uniques)\n",
    "    amenities_uniques.remove('')\n",
    "    amenities_uniques.remove(None)\n",
    "    # Only two rows have Washer / Dryer, and they both have washer and dryer\n",
    "    amenities_uniques.remove('Washer / Dryer')\n",
    "    # When 'Pets live on this property' is True, one or more from 'Cat(s)', 'Dog(s)', 'Other pet(s)' will appear\n",
    "    amenities_uniques\n",
    "\n",
    "\n",
    "    ##\n",
    "    #revise change _df name\n",
    "    ###\n",
    "\n",
    "    # Encoding amenities\n",
    "    amenities_enc = pd.DataFrame()\n",
    "    for amenity in amenities_uniques:\n",
    "        amenities_enc[amenity] = listings.amenities.str.contains(amenity)\n",
    "\n",
    "    # Rename the columns with prefix amenity_\n",
    "    amenities_enc.columns = [f\"amenity_{col}\" for col in amenities_enc.columns]\n",
    "    \n",
    "    df = pd.concat([listings, amenities_enc], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "listings = parse_listings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b0d7e-64bb-4382-98f4-c0dd42b1e309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4288e9f8-500d-4f31-a8f2-5273ea3ff7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse calendar\n",
    "def parse_calender():\n",
    "    #\n",
    "    # Data type transformation\n",
    "    #\n",
    "    # Convert date from object to datetime\n",
    "    calendar.date = pd.to_datetime(calendar.date)\n",
    "    # Convert price from object to float\n",
    "    # Convert '$' and ',' to ''\n",
    "    calendar.price = calendar.price.replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "    #\n",
    "    # Imputation\n",
    "    #\n",
    "    # Imputation -> Forward filling\n",
    "    calendar['price_ffill'] = calendar.groupby('listing_id')['price'].transform(lambda col: col.ffill())\n",
    "    \n",
    "    return calendar\n",
    "\n",
    "def avg_price(period=30, today=None):\n",
    "    # bookmark: change function name\n",
    "    # bug year, \n",
    "    \"\"\"Returns average price in the period.\n",
    "    \n",
    "    Args:\n",
    "        period (int): Valid period bre today. Default is 30.\n",
    "        today (tuple): Int in tuple, e.g., (2016, 1, 1). Default is today's date.\n",
    "        \n",
    "    Returns:\n",
    "        Pandas DataFrame\n",
    "        \n",
    "    Examples:\n",
    "        >>> avg_price()\n",
    "        etlLogger - WARNING - No records!\n",
    "        >>> avg_price(period=50)\n",
    "        etlLogger - WARNING - No records!\n",
    "        >>> avg_price(period=30, today=(2017, 1, 1))\n",
    "        ...\n",
    "        >>> avg_price(period=100, today=(2016, 8, 15))\n",
    "        ...\n",
    "        >>> avg_price(period=100, today=(2016, 8, 15))\n",
    "        etlLogger - INFO - No sufficient records!\n",
    "        ...\n",
    "    \"\"\"\n",
    "    assert isinstance(period, int) & (period > 0), \"period must be int and greater than 0!\"\n",
    "    \n",
    "    calendar = parse_calender()\n",
    "    \n",
    "    # Define today\n",
    "    today = pd.Timestamp.today().date() if today is None else pd.Timestamp(today[0], today[1], today[2]).date()\n",
    "    # Define day1\n",
    "    day1 = today - pd.DateOffset(days=period)\n",
    "    day1 = day1.date()\n",
    "    # Define valid date range\n",
    "    \"\"\"\n",
    "    valid: 2016-01-01, 2016-12-31\n",
    "    query: 30, 2017-01-31 -> 'No records!\n",
    "    query: 30, 2015-12-31 -> 'No records!\n",
    "    query: 30, 2017-01-15 -> 'No sufficient records!'\n",
    "    query: 30, 2016-01-15 -> 'No sufficient records!'\n",
    "    \"\"\"\n",
    "    record_1st, record_last = calendar.date.min().date(), calendar.date.max().date()\n",
    "    if (day1 > record_last) | (today < record_1st):\n",
    "        etlLogger.warning('No records!')\n",
    "    elif (day1 < record_1st) | (record_last < today):\n",
    "        etlLogger.info('No sufficient records!')\n",
    "    \n",
    "    \"\"\"Test == avg_price(period=30, today=(2017, 1, 1))\n",
    "    # SQLite BETWEEN -> inclusive, exclusive\n",
    "    >>> import pandasql # running on sqlite3 syntax\n",
    "\n",
    "    >>> today = pd.Timestamp(2017, 1, 1).date()\n",
    "    >>> period = 30\n",
    "    >>> day1 = today - pd.DateOffset(days=period)\n",
    "    >>> day1 = day1.date()\n",
    "\n",
    "    >>> sql=f'''\n",
    "        SELECT \n",
    "            listing_id, \n",
    "            AVG(price_ffill) AS avg_price,\n",
    "            \"{str(day1)}\" AS start_date,\n",
    "            \"{(today - pd.DateOffset(days=1)).date()}\" AS end_date\n",
    "        FROM calendar\n",
    "        WHERE date BETWEEN \"{str(day1)}\" AND \"{str(today)}\"\n",
    "        GROUP BY listing_id\n",
    "        '''\n",
    "\n",
    "    >>> pandasql.sqldf(sql, locals())\n",
    "    \"\"\"\n",
    "    filter = (day1 <= calendar.date.dt.date) & (calendar.date.dt.date < today)\n",
    "    result = calendar[filter].groupby(['listing_id'])['price_ffill'].mean().reset_index(name='avg_price')\n",
    "    result['start_date'] = day1\n",
    "    result['end_date'] = (today - pd.DateOffset(days=1)).date()\n",
    "    return result\n",
    "\n",
    "calendar = avg_price(period=30, today=(2017, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42624ed-7ea2-4efb-86d0-2bd9a59ae5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7c4bb-02b7-485b-8c86-684a38d37bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742bd280-1fdb-414f-a490-5be3c54af0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc3130-baa2-478e-8cc0-401fdad1dc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "025f7e39-7c3a-4d1a-a406-249e34625ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSELECT *\\nFROM listings\\nLEFT JOIN calendar_annual\\n    ON listings.id = calendar_annual.listing_id\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "#revise\n",
    "###\n",
    "\n",
    "\"\"\"\n",
    "SELECT *\n",
    "FROM listings\n",
    "LEFT JOIN calendar_annual\n",
    "    ON listings.id = calendar_annual.listing_id\n",
    "\"\"\"\n",
    "\n",
    "#calendar_annual.set_index(['listing_id'], inplace=True)\n",
    "#listings.set_index(['id'], inplace=True)\n",
    "\n",
    "#df = listings.set_index(['id']).join(calendar_annual.set_index(['listing_id']), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8299922d-08cb-44d7-bf98-6fa686c4bd83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5fcea-f901-4578-b6c3-598b6cdf0365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8924aa-8a69-4c37-b5da-994cc9efb136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990673e4-286b-4e0e-81f8-cde00a244c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8115062-2716-4b69-bd5f-28a7733cbd12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
