{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f493af-0d5f-4109-87ef-2fe231e026fd",
   "metadata": {},
   "source": [
    "# Kaggle Seattle Airbnb\n",
    "\n",
    "## Project Info\n",
    "\n",
    "### Author Info\n",
    "\n",
    "- Author: [Zacks Shen](https://www.linkedin.com/in/zacks-shen/)\n",
    "- Contributor: [Kevin Chu](https://www.linkedin.com/in/yen-duo-chu/)\n",
    "\n",
    "---\n",
    "\n",
    "### GitHub\n",
    "\n",
    "- [Kaggle-Seattle-Airbnb](https://github.com/ZacksAmber/Kaggle-Seattle-Airbnb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c17d1-bc67-4abb-ac81-433983138edf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8226e55-724c-4403-b8a2-febed74bc158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as mt\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "'''Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "px.defaults.width = 1200\n",
    "px.defaults.height = 800\n",
    "# plotly.io Settings for both plotly.graph_objects and plotly.express\n",
    "pio.templates.default = \"plotly_white\" # \"plotly\", \"plotly_white\", \"plotly_dark\", \"ggplot2\", \"seaborn\", \"simple_white\", \"none\"\n",
    "pio.kaleido.scope.default_format = 'svg'\n",
    "pio.kaleido.scope.default_scale = 1\n",
    "'''\n",
    "\n",
    "# Data Preprocessing - Standardization, Encoding, Imputation\n",
    "from sklearn.preprocessing import StandardScaler # Standardization\n",
    "from sklearn.preprocessing import Normalizer # Normalization\n",
    "from sklearn.preprocessing import OneHotEncoder # One-hot Encoding\n",
    "from sklearn.preprocessing import OrdinalEncoder # Ordinal Encoding\n",
    "from category_encoders import MEstimateEncoder # Target Encoding\n",
    "from sklearn.preprocessing import PolynomialFeatures # Create Polynomial Features\n",
    "from sklearn.impute import SimpleImputer # Imputation\n",
    "\n",
    "# Exploratory Data Analysis - Feature Engineering\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modeling - ML Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Modeling - Algorithms\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "#from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# ML - Evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# ML - Tuning\n",
    "import optuna\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Settings\n",
    "# Settings for Seaborn\n",
    "sns.set_theme(context='notebook', style='ticks', palette=\"bwr_r\", font_scale=0.7, rc={\"figure.dpi\":240, 'savefig.dpi':240})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239dbdee-e119-4525-8838-b783c46549a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/201601/reviews.csv\n",
      "./data/201601/listings_kfold.csv\n",
      "./data/201601/listings.csv\n",
      "./data/201601/calendar.csv\n",
      "./data/201601/.ipynb_checkpoints/listings_kfold-checkpoint.csv\n",
      "./data/201601/.ipynb_checkpoints/calendar-checkpoint.csv\n",
      "./data/201601/.ipynb_checkpoints/listings-checkpoint.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "kaggle_project = 'seattle'\n",
    "# Import dataset from local directory './data' or from Kaggle\n",
    "data_dir = ('./data/201601' if os.path.exists('data') else f'/kaggle/input/{kaggle_project}')\n",
    "\n",
    "# print all files in data_dir\n",
    "for dirname, _, filenames in os.walk(data_dir):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Import three datasets\n",
    "reviews = pd.read_csv(f'{data_dir}/reviews.csv')\n",
    "calendar = pd.read_csv(f'{data_dir}/calendar.csv')\n",
    "listings = pd.read_csv(f'{data_dir}/listings_kfold.csv') if os.path.exists(f'{data_dir}/listings_kfold.csv') else pd.read_csv(f'{data_dir}/listings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a36e4b-a75b-4ff2-acd6-1e8c8e9ad055",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cross-Validation KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba0523b-0853-4b15-84ba-9030e78e3781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_listings_kfold():\n",
    "    # Mark the train dataset with kfold = 5\n",
    "    listings = pd.read_csv(f'{data_dir}/listings.csv')\n",
    "    if os.path.exists(f'{data_dir}/listings_kfold.csv'):\n",
    "        os.remove(f'{data_dir}/listings_kfold.csv')\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(X=listings)):\n",
    "        listings.loc[valid_idx, \"kfold\"] = fold\n",
    "\n",
    "    listings.to_csv(f'{data_dir}/listings_kfold.csv', index=False)\n",
    "#generate_listings_kfold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0d32db-a938-4a2d-afc7-68d3daac41d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>241032</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>953595</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3308979</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7421966</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>278830</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  kfold\n",
       "0   241032    0.0\n",
       "1   953595    4.0\n",
       "2  3308979    2.0\n",
       "3  7421966    3.0\n",
       "4   278830    4.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After assigning kfold\n",
    "# If error, run the above function then re-load listings_kfold.csv\n",
    "listings.loc[:, ['id', 'kfold']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e67d835-fc4b-4750-a1f0-960369c125b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ETL Pipeline\n",
    "\n",
    "The ETL pipeline provides data transformation and formatting. Thus, we can calculate the data and perform machine learning with the correct data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72e10c0e-4aad-4c90-8d20-11ee52affda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETL_pipeline:\n",
    "    def __init__(self, data_frame):\n",
    "        self.df = data_frame\n",
    "    \n",
    "    # Data type transformation\n",
    "    def _transformation(self, data_frame):\n",
    "        df = data_frame\n",
    "        # Convert dollar columns from object to float\n",
    "        # Remove '$' and ','\n",
    "        dollar_cols = ['price', 'weekly_price', 'monthly_price', 'extra_people', 'security_deposit', 'cleaning_fee']\n",
    "        for dollar_col in dollar_cols:\n",
    "            df[dollar_col] = df[dollar_col].replace('[\\$,]', '', regex=True).astype(float)\n",
    "        # Convert dollar columns from object to float\n",
    "        # Remove '%'\n",
    "        percent_cols = ['host_response_rate', 'host_acceptance_rate']\n",
    "        for percent_col in percent_cols:\n",
    "            df[percent_col] = df[percent_col].replace('%', '', regex=True).astype(float)\n",
    "\n",
    "        # Replace the following values in property_type to Unique space due to small sample size\n",
    "        unique_space = [\"Barn\",\n",
    "        \"Boat\",\n",
    "        \"Bus\",\n",
    "        \"Camper/RV\",\n",
    "        \"Treehouse\",\n",
    "        \"Campsite\",\n",
    "        \"Castle\",\n",
    "        \"Cave\",\n",
    "        \"Dome House\",\n",
    "        \"Earth house\",\n",
    "        \"Farm stay\",\n",
    "        \"Holiday park\",\n",
    "        \"Houseboat\",\n",
    "        \"Hut\",\n",
    "        \"Igloo\",\n",
    "        \"Island\",\n",
    "        \"Lighthouse\",\n",
    "        \"Plane\",\n",
    "        \"Ranch\",\n",
    "        \"Religious building\",\n",
    "        \"Shepherdâ€™s hut\",\n",
    "        \"Shipping container\",\n",
    "        \"Tent\",\n",
    "        \"Tiny house\",\n",
    "        \"Tipi\",\n",
    "        \"Tower\",\n",
    "        \"Train\",\n",
    "        \"Windmill\",\n",
    "        \"Yurt\",\n",
    "        \"Riad\",\n",
    "        \"Pension\",\n",
    "        \"Dorm\",\n",
    "        \"Chalet\"]            \n",
    "        df.property_type = df.property_type.replace(unique_space, \"Unique space\", regex=True)\n",
    "\n",
    "        # Convert 't', 'f' to 1, 0\n",
    "        tf_cols = ['host_is_superhost', 'instant_bookable', 'require_guest_profile_picture', 'require_guest_phone_verification']\n",
    "        for tf_col in tf_cols:\n",
    "            df[tf_col] = df[tf_col].replace('f', 0, regex=True)\n",
    "            df[tf_col] = df[tf_col].replace('t', 1, regex=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Parse listings\n",
    "    def parse_listings(self):\n",
    "        \"\"\"Parse listings.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        df = self._transformation(df)\n",
    "        return df\n",
    "    \n",
    "    def parse_reviews(self):\n",
    "        \"\"\"Parse reviews.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        df.date = pd.to_datetime(df.date)\n",
    "        return df\n",
    "    \n",
    "    # Parse calendar\n",
    "    def parse_calender(self):\n",
    "        \"\"\"Paser calendar.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        # Convert date from object to datetime\n",
    "        df.date = pd.to_datetime(df.date)\n",
    "        # Convert price from object to float\n",
    "        # Convert '$' and ',' to ''\n",
    "        df.price = df.price.replace('[\\$,]', '', regex=True).astype(float)\n",
    "        \n",
    "        # Convert 't', 'f' to 1, 0\n",
    "        df['available'] = df['available'].replace('f', 0, regex=True)\n",
    "        df['available'] = df['available'].replace('t', 1, regex=True)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "678893bb-a90d-4dbc-b16f-335cd772028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = ETL_pipeline(listings).parse_listings()\n",
    "reviews = ETL_pipeline(reviews).parse_reviews()\n",
    "calendar = ETL_pipeline(calendar).parse_calender()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86d7af2-4365-4dfa-ba2d-3591aae39993",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7626cbb-2ab2-4437-81a8-daa4c9a4896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDA_demand:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def reviews_rate_vs_unavailability(self, period=30):\n",
    "        \"\"\"Calculate the booked listing from file calendar.\n",
    "\n",
    "        Args:\n",
    "            period (int): Positive integer. Default is 30.\n",
    "\n",
    "        Returns:\n",
    "            Pandas DataFrame.\n",
    "        \"\"\"\n",
    "        assert (0 < period <= 365) & isinstance(period, int), \"period must be an integer and greater than 0\"\n",
    "        self.period = period\n",
    "        \n",
    "        #\n",
    "        # Calculate review rate & unavailability\n",
    "        #\n",
    "\n",
    "        # reviews Rate: review / days\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            listing_id, \n",
    "            COUNT(listing_id) / DATEDIFF(20160104+1, MIN(date)) AS reviews_per_day\n",
    "        FROM reviews\n",
    "        GROUP BY listing_id\n",
    "        \"\"\"\n",
    "        # Extract the first reviews date for each listing\n",
    "        func = lambda df: pd.Series({'first_day': df.date.min()})\n",
    "        df_reviews_per_day = pd.DataFrame(reviews.groupby('listing_id').apply(func))\n",
    "        # Define last scraped date\n",
    "        last_scraped = listings.last_scraped.unique()[0]\n",
    "        last_scraped = pd.Timestamp(last_scraped)\n",
    "        df_reviews_per_day['last_day'] = last_scraped + pd.DateOffset(days=1)\n",
    "        # Calculate the datediff\n",
    "        df_reviews_per_day['datediff'] = df_reviews_per_day.last_day - df_reviews_per_day.first_day\n",
    "        df_reviews_per_day['datediff'] = df_reviews_per_day['datediff'].dt.days\n",
    "        # Calculate the reviews Rate\n",
    "        df_reviews_per_day['reviews_per_day'] = reviews.groupby('listing_id').size() / df_reviews_per_day['datediff']\n",
    "\n",
    "        \"\"\"\n",
    "        SELECT listing_id, SUM(IF(available = 0, 1, 0))\n",
    "        FROM calendar\n",
    "        WHERE DATEDIFF(date, 20160104) <= period\n",
    "        GROUP BY listing_id\n",
    "        \"\"\"\n",
    "        last_day = last_scraped + pd.DateOffset(days=period-1)\n",
    "        filter = calendar.date <= (last_day)\n",
    "        func = lambda df: pd.Series({f'unavailability_{period}_unscaled': sum(df.available == 0)}) # Scaling available to day scale\n",
    "        df_unavailability = pd.DataFrame(calendar[filter].groupby('listing_id').apply(func))\n",
    "        df_unavailability[f'unavailability_{period}'] = df_unavailability[f'unavailability_{period}_unscaled'] / period\n",
    "        #df_unavailability['first_day'] = last_scraped\n",
    "        #df_unavailability['last_day'] = last_day\n",
    "        self.df_unavailability = df_unavailability\n",
    "        \n",
    "        # Join two tables\n",
    "        df_unavailability_reviews = df_unavailability.join(df_reviews_per_day, how='left')\n",
    "        df_unavailability_reviews.reviews_per_day.fillna(value=0, inplace=True)\n",
    "        #df_unavailability_reviews.loc[:, [f'unavailability_{period}_unscaled', f'unavailability_{period}', 'reviews_per_day']]\n",
    "        \n",
    "        # Find outliers (unavailable rather than booked)\n",
    "        # Extrat quantiles\n",
    "        reviews_rate_25 = df_unavailability_reviews.reviews_per_day.quantile(q=0.25, interpolation='higher')\n",
    "        unavailability_75 = df_unavailability_reviews[f'unavailability_{period}'].quantile(q=0.75, interpolation='higher')\n",
    "        # Low reviews rate: 0.010376\n",
    "        filter1 = df_unavailability_reviews.reviews_per_day < reviews_rate_25\n",
    "        # High unavailability: 0.660274\n",
    "        filter2 = df_unavailability_reviews[f'unavailability_{period}'] > unavailability_75\n",
    "\n",
    "        outliers = df_unavailability_reviews[filter1 & filter2]\n",
    "        df_unavailability_reviews['demand'] = df_unavailability_reviews[f'unavailability_{period}_unscaled']\n",
    "        df_unavailability_reviews.loc[outliers.index, 'demand'] = period - df_unavailability_reviews.loc[outliers.index, 'demand']\n",
    "        \n",
    "        self.outliers = outliers\n",
    "        self.df_unavailability_reviews = df_unavailability_reviews\n",
    "        \n",
    "        return self.df_unavailability_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d265f99d-045e-4ad3-b194-45ecd2a239a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML_pipeline:\n",
    "    \"\"\"ML Pipeline for listings.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_frame, features, target, days=365):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            data_frame (Pandas DataFrame): listings.\n",
    "            features (list): The Machine Learning features.\n",
    "            target (str): price\n",
    "            days (int): The days after 2016-01-04 for calculating demand.\n",
    "        \"\"\"\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\") # ignore target encoding warnings\n",
    "        \n",
    "        # Get demand\n",
    "        demand = EDA_demand().reviews_rate_vs_unavailability(days)\n",
    "        # The index will change to id\n",
    "        data_frame = data_frame.set_index('id').join(demand['demand'], how='inner')\n",
    "        \n",
    "        features.append(target)\n",
    "        data_frame = data_frame[features]\n",
    "        \n",
    "        # Encode amenities\n",
    "        data_frame = self._encode_amentities(data_frame)\n",
    "        data_frame.pop('amenities')\n",
    "        \n",
    "        self.data_frame = data_frame\n",
    "        \n",
    "    # encode amentities\n",
    "    def _encode_amentities(self, data_frame):\n",
    "        # Replace amenities from {}\" to ''\n",
    "        data_frame.amenities.replace('[{}\"]', '', regex=True, inplace=True)\n",
    "        # Split amenities with ,\n",
    "        amenities = data_frame.amenities.str.split(',', expand=True)\n",
    "        \n",
    "        \"\"\"All amenities\n",
    "        '24-Hour Check-in',\n",
    "        'Air Conditioning',\n",
    "        'Breakfast',\n",
    "        'Buzzer/Wireless Intercom',\n",
    "        'Cable TV',\n",
    "        'Carbon Monoxide Detector',\n",
    "        'Cat(s)',\n",
    "        'Dog(s)',\n",
    "        'Doorman',\n",
    "        'Dryer',\n",
    "        'Elevator in Building',\n",
    "        'Essentials',\n",
    "        'Family/Kid Friendly',\n",
    "        'Fire Extinguisher',\n",
    "        'First Aid Kit',\n",
    "        'Free Parking on Premises',\n",
    "        'Gym',\n",
    "        'Hair Dryer',\n",
    "        'Hangers',\n",
    "        'Heating',\n",
    "        'Hot Tub',\n",
    "        'Indoor Fireplace',\n",
    "        'Internet',\n",
    "        'Iron',\n",
    "        'Kitchen',\n",
    "        'Laptop Friendly Workspace',\n",
    "        'Lock on Bedroom Door',\n",
    "        'Other pet(s)',\n",
    "        'Pets Allowed',\n",
    "        'Pets live on this property',\n",
    "        'Pool',\n",
    "        'Safety Card',\n",
    "        'Shampoo',\n",
    "        'Smoke Detector',\n",
    "        'Smoking Allowed',\n",
    "        'Suitable for Events',\n",
    "        'TV',\n",
    "        'Washer',\n",
    "        'Washer / Dryer',\n",
    "        'Wheelchair Accessible',\n",
    "        'Wireless Internet'\n",
    "        \"\"\"\n",
    "\n",
    "        # For each col, extract the unique amenities\n",
    "        amenities_uniques = []\n",
    "        for col in amenities.columns:\n",
    "            amenities_uniques += list(amenities[col].unique())\n",
    "\n",
    "        # Remove the duplicate values\n",
    "        amenities_uniques = set(amenities_uniques)\n",
    "        amenities_uniques.remove('')\n",
    "        amenities_uniques.remove(None)\n",
    "        # Only two rows have Washer / Dryer, and they both have washer and dryer\n",
    "        amenities_uniques.remove('Washer / Dryer')\n",
    "        # When 'Pets live on this property' is True, one or more from 'Cat(s)', 'Dog(s)', 'Other pet(s)' will appear\n",
    "\n",
    "        # Encoding amenities\n",
    "        amenities_enc = pd.DataFrame()\n",
    "        for amenity in amenities_uniques:\n",
    "            amenities_enc[amenity] = data_frame.amenities.str.contains(amenity, regex=False)\n",
    "\n",
    "        # Rename the columns with prefix amenity_\n",
    "        amenities_enc.columns = [f\"amenity_{col}\" for col in amenities_enc.columns]\n",
    "        \n",
    "        # Concat encoded amenities and data_frame\n",
    "        data_frame = pd.concat([data_frame, amenities_enc], axis=1)\n",
    "\n",
    "        return data_frame\n",
    "\n",
    "    def _imputation(self, X_train, X_valid, y_train, y_valid):\n",
    "        X_train, X_valid, y_train, y_valid = X_train.copy(), X_valid.copy(), y_train.copy(), y_valid.copy()\n",
    "        \n",
    "        # Zero imputation\n",
    "        # Reason:\n",
    "        zero_imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        zero_features = ['reviews_per_month', 'host_response_rate', 'host_is_superhost', 'security_deposit', 'cleaning_fee']\n",
    "        X_train_zero_imp = pd.DataFrame(zero_imp.fit_transform(X_train[zero_features]))\n",
    "        X_valid_zero_imp = pd.DataFrame(zero_imp.transform(X_valid[zero_features]))\n",
    "        X_train_zero_imp.columns = zero_features\n",
    "        X_valid_zero_imp.columns = zero_features\n",
    "        X_train_zero_imp.index = X_train.index\n",
    "        X_valid_zero_imp.index = X_valid.index\n",
    "        X_train_zero_imp = X_train_zero_imp.astype(float)\n",
    "        X_valid_zero_imp = X_valid_zero_imp.astype(float)\n",
    "        \n",
    "        # Mean imputation\n",
    "        # Reason:\n",
    "        mean_imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        mean_features = ['host_acceptance_rate', 'review_scores_accuracy', 'review_scores_checkin', \n",
    "                         'review_scores_value', 'review_scores_location', 'review_scores_cleanliness', \n",
    "                         'review_scores_communication', 'review_scores_rating']\n",
    "        X_train_mean_imp = pd.DataFrame(mean_imp.fit_transform(X_train[mean_features]))\n",
    "        X_valid_mean_imp = pd.DataFrame(mean_imp.transform(X_valid[mean_features]))\n",
    "        X_train_mean_imp.columns = mean_features\n",
    "        X_valid_mean_imp.columns = mean_features\n",
    "        X_train_mean_imp.index = X_train.index\n",
    "        X_valid_mean_imp.index = X_valid.index\n",
    "        X_train_mean_imp = X_train_mean_imp.astype(float)\n",
    "        X_valid_mean_imp = X_valid_mean_imp.astype(float)\n",
    "        \n",
    "        # Mode imputation\n",
    "        # Reason: \n",
    "        mode_imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "        mode_features = ['bathrooms', 'bedrooms', 'beds', 'property_type']\n",
    "        X_train_mode_imp = pd.DataFrame(mode_imp.fit_transform(X_train[mode_features]))        \n",
    "        X_valid_mode_imp = pd.DataFrame(mode_imp.transform(X_valid[mode_features]))\n",
    "        X_train_mode_imp.columns = mode_features\n",
    "        X_valid_mode_imp.columns = mode_features\n",
    "        X_train_mode_imp.index = X_train.index\n",
    "        X_valid_mode_imp.index = X_valid.index\n",
    "        X_train_mode_imp[['bathrooms', 'bedrooms', 'beds']] = X_train_mode_imp[['bathrooms', 'bedrooms', 'beds']].astype(int)\n",
    "        X_valid_mode_imp[['bathrooms', 'bedrooms', 'beds']] = X_valid_mode_imp[['bathrooms', 'bedrooms', 'beds']].astype(int)\n",
    "        \n",
    "        # Replace the unimputated columns\n",
    "        for feature in zero_features:\n",
    "            X_train[feature] = X_train_zero_imp[feature]\n",
    "            X_valid[feature] = X_valid_zero_imp[feature]\n",
    "        \n",
    "        for feature in mean_features:\n",
    "            X_train[feature] = X_train_mean_imp[feature]\n",
    "            X_valid[feature] = X_valid_mean_imp[feature]\n",
    "\n",
    "        for feature in mode_features:\n",
    "            X_train[feature] = X_train_mode_imp[feature]\n",
    "            X_valid[feature] = X_valid_mode_imp[feature]\n",
    "        \n",
    "        return X_train, X_valid, y_train, y_valid\n",
    "    \n",
    "    def _one_hot_encoding(self, X_train, X_valid, y_train, y_valid):\n",
    "        X_train, X_valid, y_train, y_valid = X_train.copy(), X_valid.copy(), y_train.copy(), y_valid.copy()\n",
    "        \n",
    "        oe_enc_features = ['cancellation_policy', 'require_guest_profile_picture', 'require_guest_phone_verification', \n",
    "                               'neighbourhood_group_cleansed', 'property_type', 'instant_bookable', 'room_type', 'bed_type']\n",
    "        \n",
    "        oe = OrdinalEncoder()\n",
    "        X_train[oe_enc_features] = oe.fit_transform(X_train[oe_enc_features])\n",
    "        X_valid[oe_enc_features] = oe.transform(X_valid[oe_enc_features])\n",
    "    \n",
    "        return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "    def _target_encoding(self, X_train, X_valid, y_train, y_valid):\n",
    "        X_train, X_valid, y_train, y_valid = X_train.copy(), X_valid.copy(), y_train.copy(), y_valid.copy()\n",
    "        \n",
    "        target_enc_features = ['cancellation_policy', 'require_guest_profile_picture', 'require_guest_phone_verification', \n",
    "                               'neighbourhood_group_cleansed', 'property_type', 'instant_bookable', 'room_type', 'bed_type']\n",
    "        \n",
    "        # Create the encoder instance. Choose m to control noise.\n",
    "        target_enc = MEstimateEncoder(cols=target_enc_features, m=5.0)\n",
    "        X_train = target_enc.fit_transform(X_train, y_train)\n",
    "        X_valid = target_enc.transform(X_valid)\n",
    "        \n",
    "        return X_train, X_valid, y_train, y_valid\n",
    "    \n",
    "    def getData(self, kfold, target_encoding=True):\n",
    "        data_frame = self.data_frame.copy()\n",
    "        \n",
    "        # Split train and valid\n",
    "        X_train = data_frame[data_frame.kfold != kfold]\n",
    "        X_valid = data_frame[data_frame.kfold == kfold]\n",
    "        y_train = X_train.pop('price')\n",
    "        y_valid = X_valid.pop('price')\n",
    "        \n",
    "        # Imputation\n",
    "        X_train, X_valid, y_train, y_valid = self._imputation(X_train, X_valid, y_train, y_valid)\n",
    "        \n",
    "        # Target Encoding\n",
    "        if target_encoding:\n",
    "            X_train, X_valid, y_train, y_valid = self._target_encoding(X_train, X_valid, y_train, y_valid)\n",
    "        else:\n",
    "            X_train, X_valid, y_train, y_valid = self._one_hot_encoding(X_train, X_valid, y_train, y_valid)\n",
    "        \n",
    "        return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89d54d-73e6-4b09-a6b0-253066c3363b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "## Model Tuning\n",
    "\n",
    "The Hyperparameter tuning platform I used is [Optuna](https://optuna.org/).<br>\n",
    "I implemented a logger to write the tuning results in the local log file.<br>\n",
    "After all tunings are finished, the program will sent an email to my mailbox with the best hyperparameters.\n",
    "- **To enable this feature**, go to [configure your gmail first](#gmail-configuration).\n",
    "\n",
    "P.S: If your computer does not support GPU accleration, uncomment code `For CPU` and comment code `FOR GPU`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaeea19-26d8-4379-9e23-aed68761491e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Define Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "311e399a-5d58-4951-b691-694872ef0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Define logger\n",
    "logger = logging.getLogger('ML')\n",
    "\n",
    "# Set level for logger\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Define the handler and formatter for file logging\n",
    "log_file = 'ML'\n",
    "fileHandler = logging.FileHandler(f'{log_file}.log') # Define FileHandler\n",
    "fileHandler.setLevel(logging.INFO) # Set level\n",
    "fileFormatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s') # Define formatter\n",
    "fileHandler.setFormatter(fileFormatter) # Set formatter\n",
    "logger.addHandler(fileHandler) # Add handler to logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8954df-e8c1-4ec5-badc-54aa75c8ebd7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Define Features for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18055020-1c62-4edf-903e-0402d66cdcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sheet id and base url\n",
    "sheet_id = \"1M_qah-ym6O8vDcSmoKAP-lbZRPHUey83R_DJaW3LXfs\"\n",
    "base_url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet=\"\n",
    "\n",
    "# Load metadata for three datasets\n",
    "listings_metadata = pd.read_csv(base_url+\"listings\")\n",
    "calendar_metadata = pd.read_csv(base_url+\"calendar\")\n",
    "reviews_metadata = pd.read_csv(base_url+\"reviews\")\n",
    "\n",
    "amenities = ['amenity_Washer', 'amenity_Air Conditioning', 'amenity_TV',\n",
    "             'amenity_Kitchen', 'amenity_Wheelchair Accessible',\n",
    "             'amenity_Free Parking on Premises', 'amenity_Doorman',\n",
    "             'amenity_Cable TV', 'amenity_Smoke Detector',\n",
    "             'amenity_Pets live on this property', 'amenity_Internet',\n",
    "             'amenity_Hangers', 'amenity_Family/Kid Friendly',\n",
    "             'amenity_First Aid Kit', 'amenity_Indoor Fireplace', 'amenity_Gym',\n",
    "             'amenity_Suitable for Events', 'amenity_Breakfast', 'amenity_Cat(s)',\n",
    "             'amenity_Lock on Bedroom Door', 'amenity_Smoking Allowed',\n",
    "             'amenity_Dog(s)', 'amenity_Shampoo', 'amenity_Hair Dryer',\n",
    "             'amenity_Carbon Monoxide Detector', 'amenity_Wireless Internet',\n",
    "             'amenity_Hot Tub', 'amenity_Safety Card',\n",
    "             'amenity_Buzzer/Wireless Intercom', 'amenity_Pool',\n",
    "             'amenity_Elevator in Building', 'amenity_Pets Allowed',\n",
    "             'amenity_Fire Extinguisher', 'amenity_Other pet(s)',\n",
    "             'amenity_Laptop Friendly Workspace', 'amenity_Essentials',\n",
    "             'amenity_Iron', 'amenity_Dryer', 'amenity_24-Hour Check-in',\n",
    "             'amenity_Heating']\n",
    "    \n",
    "# ML1 + ML2\n",
    "ml1 = listings_metadata[listings_metadata.ML == 1].Label.to_list()\n",
    "useless_features = ['availability_30', 'availability_60', 'availability_90', 'availability_365', 'first_review', 'last_review', 'amenities']\n",
    "for useless_feature in useless_features:\n",
    "    ml1.remove(useless_feature)\n",
    "ml2 = listings_metadata[listings_metadata.ML == 2].Label.to_list()\n",
    "ml2.append('demand')\n",
    "ml2 = ml1 + ml2 + amenities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d08a2cb-2c7a-4822-b1a5-d6debc682437",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Tuning Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1960cb02-0afc-45b3-a25f-0dff69ade1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence Optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dbb6054-5d9f-48b7-8f2d-9d9636408ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of trails\n",
    "n_trials = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df5587-2cb2-4516-b335-471e8ff59f53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Model Tuning: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ae5bbea-39ec-4ebd-b549-b3dab54952b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Modeling tuning with Target encoding.\n",
    "    \"\"\"\n",
    "    features = ['host_acceptance_rate', 'neighbourhood_group_cleansed', 'property_type', 'room_type',\n",
    "                'bathrooms', 'bedrooms', 'beds', 'bed_type', 'number_of_reviews', 'review_scores_rating',\n",
    "                'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n",
    "                'review_scores_location', 'review_scores_value', 'reviews_per_month', 'host_response_rate', 'host_is_superhost', \n",
    "                'accommodates', 'security_deposit', 'cleaning_fee', 'guests_included', 'extra_people', 'minimum_nights', \n",
    "                'maximum_nights', 'instant_bookable', 'cancellation_policy', 'require_guest_profile_picture', \n",
    "                'require_guest_phone_verification', 'amenities', 'demand', 'kfold']\n",
    "\n",
    "    ml_pipeline = ML_pipeline(data_frame=listings, features=features, target='price')\n",
    "    \n",
    "    RMSE_AVG = []\n",
    "    for kfold in range(5):\n",
    "        X_train, X_valid, y_train, y_valid = ml_pipeline.getData(kfold=kfold, target_encoding=True)\n",
    "        X_train, X_valid = X_train[ml2], X_valid[ml2]\n",
    "        \n",
    "        # Hyperparameters for XGBoost\n",
    "        xgb_params = {\n",
    "            'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "            'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "            'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0),\n",
    "            'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0),\n",
    "            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n",
    "            'subsample': trial.suggest_float(\"subsample\", 0.1, 1.0),\n",
    "            'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 10000),\n",
    "            'max_depth': trial.suggest_int(\"max_depth\", 1, 7),\n",
    "            'random_state': trial.suggest_categorical('random_state', [0, 42, 2021]),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)\n",
    "        }\n",
    "\n",
    "        \n",
    "        # For GPU\n",
    "        model = XGBRegressor(\n",
    "                tree_method='gpu_hist',\n",
    "                gpu_id=0,\n",
    "                predictor='gpu_predictor',\n",
    "                **xgb_params)\n",
    "        \n",
    "        '''\n",
    "        # For CPU\n",
    "        model = XGBRegressor(**xgb_params)\n",
    "        '''\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train, \n",
    "            early_stopping_rounds=300,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            verbose=5000\n",
    "        )\n",
    "        \n",
    "        valid_preds = model.predict(X_valid)\n",
    "        RMSE = mean_squared_error(y_valid, valid_preds, squared=False)\n",
    "        RMSE_AVG.append(RMSE)\n",
    "    \n",
    "    return np.mean(RMSE_AVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf3ca931-1ecc-48c7-bcfe-9620a1f56eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:146.89149\n",
      "[1240]\tvalidation_0-rmse:49.49187\n",
      "[0]\tvalidation_0-rmse:154.56607\n",
      "[803]\tvalidation_0-rmse:60.99924\n",
      "[0]\tvalidation_0-rmse:142.88559\n",
      "[620]\tvalidation_0-rmse:58.63733\n",
      "[0]\tvalidation_0-rmse:149.67696\n",
      "[1708]\tvalidation_0-rmse:56.09981\n",
      "[0]\tvalidation_0-rmse:148.25832\n",
      "[601]\tvalidation_0-rmse:50.44122\n",
      "[0]\tvalidation_0-rmse:144.33707\n",
      "[1362]\tvalidation_0-rmse:54.90929\n",
      "[0]\tvalidation_0-rmse:151.35757\n",
      "[1870]\tvalidation_0-rmse:64.23412\n",
      "[0]\tvalidation_0-rmse:139.70265\n",
      "[1815]\tvalidation_0-rmse:60.93744\n",
      "[0]\tvalidation_0-rmse:146.43550\n",
      "[1836]\tvalidation_0-rmse:61.31049\n",
      "[0]\tvalidation_0-rmse:145.27843\n",
      "[1708]\tvalidation_0-rmse:52.91108\n",
      "CPU times: user 5min 58s, sys: 23.5 s, total: 6min 21s\n",
      "Wall time: 34.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study = optuna.create_study(direction='minimize', study_name=f'XGBoost {n_trials} trails')\n",
    "study.optimize(objective, n_trials=n_trials, show_progress_bar=False) # set n_triasl\n",
    "\n",
    "logger.info(f\"Study name: {study.study_name}\")\n",
    "logger.info(f\"Best value: {study.best_value}\")\n",
    "logger.info(f\"Best paras: {study.best_params}\")\n",
    "logger.info(\"Mission Complete! --------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89179dbe-01ef-48ad-a701-4b32e096eac9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Model Tuning: LigtGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6ec5677-b69f-42ac-b756-7bb63dfda66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Modeling tuning with Target encoding.\n",
    "    \"\"\"\n",
    "    features = ['host_acceptance_rate', 'neighbourhood_group_cleansed', 'property_type', 'room_type',\n",
    "                'bathrooms', 'bedrooms', 'beds', 'bed_type', 'number_of_reviews', 'review_scores_rating',\n",
    "                'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n",
    "                'review_scores_location', 'review_scores_value', 'reviews_per_month', 'host_response_rate', 'host_is_superhost', \n",
    "                'accommodates', 'security_deposit', 'cleaning_fee', 'guests_included', 'extra_people', 'minimum_nights', \n",
    "                'maximum_nights', 'instant_bookable', 'cancellation_policy', 'require_guest_profile_picture', \n",
    "                'require_guest_phone_verification', 'amenities', 'demand', 'kfold']\n",
    "\n",
    "    ml_pipeline = ML_pipeline(data_frame=listings, features=features, target='price')\n",
    "    \n",
    "    RMSE_AVG = []\n",
    "    for kfold in range(5):\n",
    "        X_train, X_valid, y_train, y_valid = ml_pipeline.getData(kfold=kfold, target_encoding=True)\n",
    "        X_train, X_valid = X_train[ml2], X_valid[ml2]\n",
    "        \n",
    "        # Hyperparameters for LightGBM\n",
    "        lgb_params = {\n",
    "            'random_state': trial.suggest_categorical('random_state', [0, 42, 2021]),\n",
    "            'num_iterations': trial.suggest_int('num_iterations', 100, 10000),\n",
    "            'learning_rate': trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 7),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 2, 100),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 100, 2000),\n",
    "            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.01, 0.99),\n",
    "            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.01, 0.99),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        }\n",
    "\n",
    "        \n",
    "        # For GPU\n",
    "        model = LGBMRegressor(\n",
    "                    device='gpu',\n",
    "                    gpu_platform_id=0,\n",
    "                    gpu_device_id=0,\n",
    "                    n_jobs=-1,\n",
    "                    metric='rmse',\n",
    "                    **lgb_params\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        # For CPU\n",
    "        model = LGBMRegressor(**lgb_params)\n",
    "        '''\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train, \n",
    "            early_stopping_rounds=300,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            verbose=5000\n",
    "        )\n",
    "        \n",
    "        valid_preds = model.predict(X_valid)\n",
    "        RMSE = mean_squared_error(y_valid, valid_preds, squared=False)\n",
    "        RMSE_AVG.append(RMSE)\n",
    "    \n",
    "    return np.mean(RMSE_AVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8604b016-79ef-44b1-8f42-e381b818bc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.18363700027501578, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.18363700027501578\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5608115457815681, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5608115457815681\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.724698492237998, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.724698492237998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=646, min_child_samples=6 will be ignored. Current value: min_data_in_leaf=646\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.332080155979758e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.332080155979758e-05\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3159]\tvalid_0's l2: 5076.1\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.18363700027501578, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.18363700027501578\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5608115457815681, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5608115457815681\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.724698492237998, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.724698492237998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=646, min_child_samples=6 will be ignored. Current value: min_data_in_leaf=646\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.332080155979758e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.332080155979758e-05\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2873]\tvalid_0's l2: 5739.6\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.18363700027501578, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.18363700027501578\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5608115457815681, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5608115457815681\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.724698492237998, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.724698492237998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=646, min_child_samples=6 will be ignored. Current value: min_data_in_leaf=646\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.332080155979758e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.332080155979758e-05\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1370]\tvalid_0's l2: 5162.45\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.18363700027501578, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.18363700027501578\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5608115457815681, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5608115457815681\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.724698492237998, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.724698492237998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=646, min_child_samples=6 will be ignored. Current value: min_data_in_leaf=646\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.332080155979758e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.332080155979758e-05\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3291]\tvalid_0's l2: 5341.94\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.18363700027501578, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.18363700027501578\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5608115457815681, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5608115457815681\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.724698492237998, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.724698492237998\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=646, min_child_samples=6 will be ignored. Current value: min_data_in_leaf=646\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.332080155979758e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.332080155979758e-05\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1020]\tvalid_0's l2: 4471.65\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2938276689167533, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2938276689167533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9500690007218026, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9500690007218026\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002639844582711026, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002639844582711026\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=833, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=833\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.590452688441597e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.590452688441597e-08\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1666]\tvalid_0's l2: 4427.31\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2938276689167533, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2938276689167533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9500690007218026, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9500690007218026\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002639844582711026, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002639844582711026\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=833, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=833\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.590452688441597e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.590452688441597e-08\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2896]\tvalid_0's l2: 5117.76\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2938276689167533, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2938276689167533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9500690007218026, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9500690007218026\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002639844582711026, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002639844582711026\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=833, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=833\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.590452688441597e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.590452688441597e-08\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[291]\tvalid_0's l2: 4819.88\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2938276689167533, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2938276689167533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9500690007218026, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9500690007218026\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002639844582711026, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002639844582711026\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=833, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=833\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.590452688441597e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.590452688441597e-08\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1652]\tvalid_0's l2: 4692.36\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2938276689167533, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2938276689167533\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9500690007218026, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9500690007218026\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0002639844582711026, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0002639844582711026\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=833, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=833\n",
      "[LightGBM] [Warning] lambda_l1 is set=5.590452688441597e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.590452688441597e-08\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1203]\tvalid_0's l2: 4084.94\n",
      "CPU times: user 2min 23s, sys: 6.39 s, total: 2min 30s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study = optuna.create_study(direction='minimize', study_name=f'LGBoost {n_trials} trails')\n",
    "study.optimize(objective, n_trials=n_trials, show_progress_bar=False) # set n_triasl\n",
    "\n",
    "logger.info(f\"Study name: {study.study_name}\")\n",
    "logger.info(f\"Best value: {study.best_value}\")\n",
    "logger.info(f\"Best paras: {study.best_params}\")\n",
    "logger.info(\"Mission Complete! --------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559e27d-a1f3-4577-a7f0-e1b0032830fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Gmail Configuration<a id='gmail-configuration'></a>\n",
    "\n",
    "> [How to Send Emails with Gmail using Python](https://stackabuse.com/how-to-send-emails-with-gmail-using-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c15ae038-0a73-479f-9563-8c52e13b3600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmail(YOUR_GMAIL, YOUR_APP_PASSWORD, SEND_TO):\n",
    "    \"\"\"Send the ML tuning result to one or more email addresses.\n",
    "    \n",
    "    Args:\n",
    "        YOUR_GMAIL (str): Your gmail address.\n",
    "        YOUR_APP_PASSWORD (str): Your APP Password for gmail. \n",
    "        SEND_TO (str or list): The target emails.\n",
    "    \"\"\"\n",
    "    gmail_user = YOUR_GMAIL\n",
    "    gmail_password = YOUR_APP_PASSWORD # Google App Password\n",
    "\n",
    "    import smtplib\n",
    "    from email.message import EmailMessage\n",
    "\n",
    "    msg = EmailMessage()\n",
    "    msg[\"From\"] = YOUR_GMAIL\n",
    "    msg[\"Subject\"] = \"Seattle Airbnb ML Tuning\"\n",
    "    msg[\"To\"] = SEND_TO\n",
    "    msg.set_content(f\"\"\"\\\n",
    "    {n_trials} Trials are done.\n",
    "    Mission Complete!\"\"\")\n",
    "    with open('ML.log', 'rb') as f:\n",
    "        content = f.read()\n",
    "        msg.add_attachment(content, maintype='application', subtype='log', filename='ML.log')\n",
    "\n",
    "    server = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n",
    "    server.login(gmail_user, gmail_password)\n",
    "    server.send_message(msg)\n",
    "    server.close()\n",
    "#gmail(YOUR_GMAIL, YOUR_APP_PASSWORD, SEND_TO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
